{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E2EGANTTS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9dzpCSexVzUg",
        "4dSoD3OHiG-5",
        "0PdF-HRqiJXM",
        "gsFma2NQjwXJ",
        "H9JamBZvFzCS",
        "h_n9vIh_-XFM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dzpCSexVzUg"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOs78M4iV3L1"
      },
      "source": [
        "!pip install tensorflow_text\r\n",
        "!pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUXFj5H5hfld"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_text as text\r\n",
        "import tensorflow_addons as tfa\r\n",
        "import numpy as np\r\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRQYjkUNuMGY"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dSoD3OHiG-5"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PdF-HRqiJXM"
      },
      "source": [
        "### Orthogonal regularizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dHyeXwZiMX-"
      },
      "source": [
        "class OrthogonalRegularizer(tf.keras.regularizers.Regularizer):\r\n",
        "    def __init__(self, beta=1e-4, **kwargs):\r\n",
        "        super(OrthogonalRegularizer, self).__init__(**kwargs)\r\n",
        "        self.beta = beta\r\n",
        "\r\n",
        "    def call(self, inputTensor):\r\n",
        "        c = inputTensor.shape[-1]\r\n",
        "        x = tf.reshape(inputTensor, (-1, c))\r\n",
        "        orthoLoss = tf.matmul(x, x, transpose_a=True) * (1 - tf.eye(c))\r\n",
        "        outputs = self.beta * tf.norm(orthoLoss)\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsFma2NQjwXJ"
      },
      "source": [
        "### Normalized convolutional layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yzrTDq9j2vy"
      },
      "source": [
        "class SpectralConv1D(tf.keras.layers.Layer):\r\n",
        "    def __init__(self, filters, kernelSize, strides=1,\r\n",
        "                padding='same', dilation=1, activation=None,\r\n",
        "                kernelInit=tf.initializers.Orthogonal,\r\n",
        "                kernelReg=OrthogonalRegularizer(), **kwargs):\r\n",
        "        super(SpectralConv1D, self).__init__(**kwargs)\r\n",
        "        self.filters = filters\r\n",
        "        self.kernelSize = kernelSize\r\n",
        "        self.strides = strides\r\n",
        "        self.padding = padding\r\n",
        "        self.dilation = dilation\r\n",
        "        self.activation = activation\r\n",
        "        self.kernelInit = kernelInit\r\n",
        "        self.kernelReg = kernelReg\r\n",
        "        self.spectralConv = tfa.layers.SpectralNormalization(\r\n",
        "            tf.keras.layers.Conv1D(filters=self.filters, kernel_size=self.kernelSize, strides=self.strides,\r\n",
        "                                padding=self.padding, dilation_rate=self.dilation, activation=self.activation,\r\n",
        "                                kernel_initializer=self.kernelInit, kernel_regularizer=self.kernelReg))\r\n",
        "  \r\n",
        "    def call(self, inputs):\r\n",
        "        outputs = self.spectralConv(inputs)\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaMUHaki8HNj"
      },
      "source": [
        "### Normalized transpose layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i2Tm1oZ8KkZ"
      },
      "source": [
        "class SpectralConv1DTranspose(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernelSize, strides, padding='same',\n",
        "                kernelInit=tf.initializers.Orthogonal,\n",
        "                kernelReg=OrthogonalRegularizer(), **kwargs):\n",
        "        super(SpectralConv1DTranspose, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernelSize = kernelSize\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.kernelInit = kernelInit\n",
        "        self.kernelReg = kernelReg\n",
        "        self.spectralConvTranspose = tfa.layers.SpectralNormalization(\n",
        "            tf.keras.layers.Conv1DTranspose(filters=self.filters, kernel_size=self.kernelSize,\n",
        "                                            strides=self.strides, padding=self.padding,\n",
        "                                            kernel_initializer=self.kernelInit, kernel_regularizer=self.kernelReg))\n",
        "  \n",
        "    def call(self, inputs):\n",
        "        outputs = self.spectralConvTranspose(inputs)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdTyHLSyfF9J"
      },
      "source": [
        "## BERT "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lbDNSTOVpH_"
      },
      "source": [
        "class BERT(tf.keras.Model):\r\n",
        "    def __init__(self, preprocessor, encoder, **kwargs):\r\n",
        "        super(BERT, self).__init__(**kwargs)\r\n",
        "        self.preprocessor = preprocessor\r\n",
        "        self.encoder = encoder\r\n",
        "        self.preprocess = hub.KerasLayer(preprocessor)\r\n",
        "        self.encode = hub.KerasLayer(encoder)\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        outputs = self.preprocess(inputs)\r\n",
        "        outputs = self.encode(outputs)\r\n",
        "        outputs = tf.expand_dims(outputs[\"pooled_output\"], axis=-1)\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9JmsaR1fKnC"
      },
      "source": [
        "## CBHG module "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SnPf7ZZHG1e"
      },
      "source": [
        "class Conv1DBank(tf.keras.Model):\r\n",
        "    def __init__(self, channels, kernelSize, activation, isTraining, **kwargs):\r\n",
        "        super(Conv1DBank, self).__init__(**kwargs)\r\n",
        "        self.channels = channels\r\n",
        "        self.kernelSize = kernelSize\r\n",
        "        self.activation = activation\r\n",
        "        self.isTraining = isTraining\r\n",
        "        self.conv1d = tf.keras.layers.Conv1D(filters=self.channels, kernel_size=self.kernelSize,\r\n",
        "                                             activation=self.activation, padding='same')\r\n",
        "        self.batchNorm = tf.keras.layers.BatchNormalization(trainable=self.isTraining)\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        outputs = self.conv1d(inputs)\r\n",
        "        outputs = self.batchNorm(outputs)\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtHAY3q4CTU9"
      },
      "source": [
        "class CBHG(tf.keras.Model):\r\n",
        "    def __init__(self, batchSize, K, isTraining, **kwargs):\r\n",
        "        super(CBHG, self).__init__(**kwargs)\r\n",
        "        self.batchSize = batchSize\r\n",
        "        self.K = K\r\n",
        "        self.isTraining = isTraining\r\n",
        "        self.ConvBanks = [Conv1DBank(128, i, tf.nn.relu, self.isTraining) for i in range(1, self.K + 1)]\r\n",
        "        self.maxPooling = tf.keras.layers.MaxPool1D(pool_size=2, strides=1, padding='same')\r\n",
        "        self.firstProjectionConv = Conv1DBank(128, 3, tf.nn.relu, self.isTraining)\r\n",
        "        self.secondProjectionConv = Conv1DBank(128, 3, None, self.isTraining)\r\n",
        "        self.highwayNet = tf.keras.Sequential([tf.keras.layers.Dense(128, tf.nn.relu) for i in range(4)])\r\n",
        "        self.bidirectionalGRU = tf.keras.layers.Bidirectional(\r\n",
        "            tf.keras.layers.GRU(64, return_sequences=True), \r\n",
        "            backward_layer=tf.keras.layers.GRU(64, return_sequences=True, go_backwards=True))\r\n",
        "        self.encoderPreNet = tf.keras.Sequential([\r\n",
        "            tf.keras.layers.Dense(256, tf.nn.relu),\r\n",
        "            tf.keras.layers.Dropout(0.5),\r\n",
        "            tf.keras.layers.Dense(128, tf.nn.relu),\r\n",
        "            tf.keras.layers.Dropout(0.5)])\r\n",
        "        self.lastProjectionConv = Conv1DBank(1, 3, None, self.isTraining)\r\n",
        "        self.upsample = tf.keras.layers.UpSampling1D(size=400)\r\n",
        "        self.conv = tf.keras.layers.Conv1D(256, 3, padding='same')\r\n",
        "    \r\n",
        "    def call(self, inputs):\r\n",
        "        outputList = []\r\n",
        "        for convBank in self.ConvBanks:\r\n",
        "            outputList.append(convBank(inputs))\r\n",
        "        outputs = tf.keras.layers.concatenate(outputList)\r\n",
        "        outputs = self.maxPooling(outputs)\r\n",
        "        outputs = self.firstProjectionConv(outputs)\r\n",
        "        outputs = self.secondProjectionConv(outputs)\r\n",
        "        highwayOutputs = outputs + inputs\r\n",
        "        outputs = self.highwayNet(highwayOutputs)\r\n",
        "        outputs = self.bidirectionalGRU(outputs)\r\n",
        "        outputs = self.encoderPreNet(outputs)\r\n",
        "        outputs = self.lastProjectionConv(outputs)\r\n",
        "        outputs = tf.reshape(outputs, (self.batchSize, 1, 256))\r\n",
        "        discOutputs = outputs\r\n",
        "        outputs = self.upsample(outputs)\r\n",
        "        genOutputs = self.conv(outputs)\r\n",
        "        return genOutputs, discOutputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w22ZErkza7_V"
      },
      "source": [
        "## Generator\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIH199WIay6V"
      },
      "source": [
        "class Generator(tf.keras.Model):\r\n",
        "    def __init__(self, batchSize, isTraining,  **kwargs):\r\n",
        "        super(Generator, self).__init__(**kwargs)\r\n",
        "        self.batchSize = batchSize\r\n",
        "        self.isTraining = isTraining\r\n",
        "        self.preProcess = SpectralConv1D(filters=768, kernelSize=3)\r\n",
        "        self.generatorBlocks = [\r\n",
        "            GeneratorBlock(768, self.isTraining, 1),\r\n",
        "            GeneratorBlock(768, self.isTraining, 1),\r\n",
        "            GeneratorBlock(384, self.isTraining, 2),\r\n",
        "            GeneratorBlock(384, self.isTraining, 2),\r\n",
        "            GeneratorBlock(384, self.isTraining, 2),\r\n",
        "            GeneratorBlock(192, self.isTraining, 3),\r\n",
        "            GeneratorBlock(96, self.isTraining, 5)]\r\n",
        "        self.postProcess = SpectralConv1D(filters=1, kernelSize=3, activation='tanh')\r\n",
        "\r\n",
        "    def call(self, inputs, noise):\r\n",
        "        outputs = self.preProcess(inputs)\r\n",
        "        for gblock in self.generatorBlocks:\r\n",
        "            outputs = gblock(outputs, noise)\r\n",
        "        outputs = self.postProcess(outputs)\r\n",
        "        outputs = tf.reshape(outputs, shape=(self.batchSize, 48000, 1))\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9JamBZvFzCS"
      },
      "source": [
        "### Generator block "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaHxvHw3FwlY"
      },
      "source": [
        "class GeneratorBlock(tf.keras.Model):\r\n",
        "    def __init__(self, channels, isTraining, upsampleFactor=1, **kwargs):\r\n",
        "        super(GeneratorBlock, self).__init__(**kwargs)\r\n",
        "        self.channels = channels\r\n",
        "        self.upsampleFactor = upsampleFactor\r\n",
        "        self.isTraining = isTraining\r\n",
        "        self.firstCBN = ConditionalBatchNorm(self.isTraining)\r\n",
        "        self.firstStack = tf.keras.Sequential([\r\n",
        "            SpectralConv1DTranspose(self.channels, 3, strides=self.upsampleFactor),\r\n",
        "            SpectralConv1D(self.channels, 3)])\r\n",
        "        self.secondCBN = ConditionalBatchNorm(self.isTraining)\r\n",
        "        self.firstDilatedConv = SpectralConv1D(self.channels, 3, dilation=2)\r\n",
        "        self.residualStack = tf.keras.Sequential([\r\n",
        "            SpectralConv1DTranspose(self.channels, 3, strides=self.upsampleFactor),\r\n",
        "            SpectralConv1D(self.channels, 1)])\r\n",
        "        self.thirdCBN = ConditionalBatchNorm(self.isTraining)\r\n",
        "        self.secondDilatedConv = SpectralConv1D(self.channels, 3, dilation=4)\r\n",
        "        self.fourthCBN = ConditionalBatchNorm(self.isTraining)\r\n",
        "        self.finalDilatedConv = SpectralConv1D(self.channels, 3, dilation=8)\r\n",
        "    \r\n",
        "\r\n",
        "    def call(self, inputs, noise):\r\n",
        "        outputs = self.firstCBN(inputs, noise)\r\n",
        "        outputs = self.firstStack(outputs)\r\n",
        "        outputs = self.secondCBN(outputs, noise)\r\n",
        "        outputs = self.firstDilatedConv(outputs)\r\n",
        "        residualOutputs = self.residualStack(inputs)\r\n",
        "        outputs = outputs + residualOutputs\r\n",
        "        outputs = self.thirdCBN(outputs, noise)\r\n",
        "        outputs = self.secondDilatedConv(outputs)\r\n",
        "        outputs = self.fourthCBN(outputs, noise)\r\n",
        "        outputs = self.finalDilatedConv(outputs)\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_n9vIh_-XFM"
      },
      "source": [
        "### Conditional batch normalization + Relu "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frGKQrZ_sRo1"
      },
      "source": [
        "class ConditionalBatchNorm(tf.keras.Model):\r\n",
        "    def __init__(self, isTraining, units=1, **kwargs):\r\n",
        "        super(ConditionalBatchNorm, self).__init__(**kwargs)\r\n",
        "        self.units = units\r\n",
        "        self.isTraining = isTraining\r\n",
        "        self.randomIdx = np.random.randint(0, 128)\r\n",
        "        self.instanceNorm = tfa.layers.InstanceNormalization()\r\n",
        "        self.matrixGamma = tf.keras.layers.Dense(\r\n",
        "            self.units, trainable=self.isTraining,\r\n",
        "            kernel_initializer=tf.keras.initializers.Constant(1.0))\r\n",
        "        self.matrixBeta = tf.keras.layers.Dense(\r\n",
        "            self.units, trainable=self.isTraining,\r\n",
        "            kernel_initializer=tf.keras.initializers.Constant(0.0))\r\n",
        "        self.flatten = tf.keras.layers.Flatten()\r\n",
        "        self.relu = tf.keras.layers.ReLU()\r\n",
        "\r\n",
        "    def call(self, inputs, noise):\r\n",
        "        outputs = self.instanceNorm(inputs)\r\n",
        "        matrixGamma = self.flatten(self.matrixGamma(noise))\r\n",
        "        matrixBeta = self.flatten(self.matrixBeta(noise))\r\n",
        "        deltaGamma = matrixGamma[0][self.randomIdx]\r\n",
        "        deltaBeta = matrixBeta[0][self.randomIdx]\r\n",
        "        outputs = tf.multiply(deltaGamma, outputs) + deltaBeta\r\n",
        "        outputs = self.relu(outputs)\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ENJHhVxn-S"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcDboTIk_p3x"
      },
      "source": [
        "class Discriminator(tf.keras.Model):\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        super(Discriminator, self).__init__(**kwargs)\r\n",
        "        self.uDscriminatorStack = [\r\n",
        "            UnconditionalDiscriminator(1, (5, 3)),\r\n",
        "            UnconditionalDiscriminator(2, (5, 3)),\r\n",
        "            UnconditionalDiscriminator(4, (5, 3)),\r\n",
        "            UnconditionalDiscriminator(8, (5, 3)),\r\n",
        "            UnconditionalDiscriminator(15, (2, 2))]\r\n",
        "        self.cDiscriminatorStack = [\r\n",
        "            ConditionalDiscriminator(1, (1, 5, 3, 2, 2, 2)),\r\n",
        "            ConditionalDiscriminator(2, (1, 5, 3, 2, 2)),\r\n",
        "            ConditionalDiscriminator(4, (1, 5, 3, 2, 2)),\r\n",
        "            ConditionalDiscriminator(8, (1, 5, 3)),\r\n",
        "            ConditionalDiscriminator(15, (1, 2, 2, 2))  \r\n",
        "        ]\r\n",
        "        self.flatten = tf.keras.layers.Flatten()\r\n",
        "        self.denseStack = ([tf.keras.layers.Dense(1) for i in range(5)])\r\n",
        "        \r\n",
        "    def call(self, w1Inputs, w2Inputs, w3Inputs, w4Inputs, w5Inputs, condition):\r\n",
        "        outputs = 0\r\n",
        "        windows = [w1Inputs, w2Inputs, w3Inputs, w4Inputs, w5Inputs]\r\n",
        "        for uDisc, cDisc, window, dense in zip(self.uDscriminatorStack, self.cDiscriminatorStack, windows, self.denseStack):\r\n",
        "            outputs += dense(self.flatten(uDisc(window)) + self.flatten(cDisc(window, condition)))\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HkoPcy13wjq"
      },
      "source": [
        "### Unconditional discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpPryvQRxsz1"
      },
      "source": [
        "class UnconditionalDiscriminator(tf.keras.Model):\r\n",
        "    def __init__(self, downsampleFactor, factors, **kwargs):\r\n",
        "        super(UnconditionalDiscriminator, self).__init__(**kwargs)\r\n",
        "        self.downsampleFactor = downsampleFactor\r\n",
        "        self.factors = factors\r\n",
        "        self.reshapeNet = tf.keras.Sequential([\r\n",
        "            SpectralConv1D(filters=self.downsampleFactor, kernelSize=1),\r\n",
        "            tf.keras.layers.MaxPool1D(self.downsampleFactor, padding='same')])\r\n",
        "        self.dBlockStack = tf.keras.Sequential([\r\n",
        "            DiscriminatorBlock(64, 1),\r\n",
        "            DiscriminatorBlock(128, self.factors[0]),\r\n",
        "            DiscriminatorBlock(256, self.factors[1]),\r\n",
        "            DiscriminatorBlock(256, 1),\r\n",
        "            DiscriminatorBlock(256, 1)])\r\n",
        "        self.avgPool = tf.keras.layers.AveragePooling1D()\r\n",
        "        \r\n",
        "    def call(self, inputs):\r\n",
        "        outputs = self.reshapeNet(inputs)\r\n",
        "        outputs = self.dBlockStack(outputs)\r\n",
        "        outputs = self.avgPool(outputs)\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ZBK42yFBzt"
      },
      "source": [
        "### Conditional discriminator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7dDEt9sFGC8"
      },
      "source": [
        "class ConditionalDiscriminator(tf.keras.Model):\r\n",
        "    def __init__(self, downsampleFactor, factors, **kwargs):\r\n",
        "        super(ConditionalDiscriminator, self).__init__(**kwargs)\r\n",
        "        self.downsampleFactor = downsampleFactor\r\n",
        "        self.factors = factors\r\n",
        "        dblockList = []\r\n",
        "        dblockSize = 64\r\n",
        "        self.reshape = tf.keras.Sequential([\r\n",
        "            SpectralConv1D(filters=self.downsampleFactor, kernelSize=1),\r\n",
        "            tf.keras.layers.MaxPool1D(self.downsampleFactor, padding='same')])\r\n",
        "        for i in range(len(self.factors) - 1):\r\n",
        "            dblockList.append(DiscriminatorBlock(dblockSize, self.factors[i]))\r\n",
        "            dblockSize = dblockSize * 2\r\n",
        "        self.dblockStack = tf.keras.Sequential(dblockList)\r\n",
        "        self.condDBlock = ConditionalDBlock(dblockSize, self.factors[-1])\r\n",
        "        self.finalDBlocks = tf.keras.Sequential([\r\n",
        "            DiscriminatorBlock(dblockSize, 1),\r\n",
        "            DiscriminatorBlock(dblockSize, 1)])\r\n",
        "        self.avgPool = tf.keras.layers.AveragePooling1D()\r\n",
        "        \r\n",
        "    def call(self, inputs, condition):\r\n",
        "        outputs = self.reshape(inputs)\r\n",
        "        outputs = self.dblockStack(outputs)\r\n",
        "        outputs = self.condDBlock(outputs, condition)\r\n",
        "        outputs = self.finalDBlocks(outputs)\r\n",
        "        outputs = self.avgPool(outputs)\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EM1bYB_Xur2"
      },
      "source": [
        "#### Conditional dblock "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKTmObkOXwwP"
      },
      "source": [
        "class ConditionalDBlock(tf.keras.Model):\r\n",
        "    def __init__(self, filters, downsampleFactor, **kwargs):\r\n",
        "        super(ConditionalDBlock, self).__init__(**kwargs)\r\n",
        "        self.filters = filters\r\n",
        "        self.downsampleFactor = downsampleFactor\r\n",
        "        self.firstStack = tf.keras.Sequential([\r\n",
        "            tf.keras.layers.MaxPool1D(self.downsampleFactor, padding='same'),\r\n",
        "            tf.keras.layers.ReLU(),\r\n",
        "            SpectralConv1D(filters=self.filters, kernelSize=3)])\r\n",
        "        self.featureConv = SpectralConv1D(filters=self.filters, kernelSize=1)\r\n",
        "        self.secondStack = tf.keras.Sequential([\r\n",
        "            tf.keras.layers.ReLU(),\r\n",
        "            SpectralConv1D(filters=self.filters, kernelSize=3, dilation=2)])\r\n",
        "        self.residualStack = tf.keras.Sequential([\r\n",
        "            SpectralConv1D(filters=self.filters, kernelSize=1),\r\n",
        "            tf.keras.layers.MaxPool1D(self.downsampleFactor, padding='same')])\r\n",
        "\r\n",
        "    def call(self, inputs, condition):\r\n",
        "        outputs = self.firstStack(inputs)\r\n",
        "        featureOutputs = self.featureConv(condition)\r\n",
        "        outputs = outputs + featureOutputs\r\n",
        "        outputs = self.secondStack(outputs)\r\n",
        "        residualOutputs = self.residualStack(inputs)\r\n",
        "        outputs = outputs + residualOutputs\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LKbyyqt33Sm"
      },
      "source": [
        "### Discriminator block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ppTnRB395T"
      },
      "source": [
        "class DiscriminatorBlock(tf.keras.Model):\r\n",
        "    def __init__(self, filters, downsampleFactor, **kwargs):\r\n",
        "        super(DiscriminatorBlock, self).__init__(**kwargs)\r\n",
        "        self.filters = filters\r\n",
        "        self.downsampleFactor = downsampleFactor\r\n",
        "        self.stack = tf.keras.Sequential([\r\n",
        "            tf.keras.layers.MaxPool1D(self.downsampleFactor, padding='same'),\r\n",
        "            tf.keras.layers.ReLU(),\r\n",
        "            SpectralConv1D(filters=self.filters, kernelSize=3, activation=tf.nn.relu),\r\n",
        "            SpectralConv1D(filters=self.filters, kernelSize=3, activation=tf.nn.relu, dilation=2)])\r\n",
        "        self.residualStack = tf.keras.Sequential([\r\n",
        "            SpectralConv1D(filters=self.filters, kernelSize=3),\r\n",
        "            tf.keras.layers.MaxPool1D(self.downsampleFactor, padding='same')])\r\n",
        "        \r\n",
        "    def call(self, inputs):\r\n",
        "        outputs = self.stack(inputs)\r\n",
        "        residualOutputs = self.residualStack(inputs)\r\n",
        "        outputs = residualOutputs + outputs\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBhSOhcW49z0"
      },
      "source": [
        "## DiscriminatorTest "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVEs4-ot5Gf2"
      },
      "source": [
        "class DiscriminatorTest(tf.keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DiscriminatorTest, self).__init__(**kwargs)\n",
        "        self.uDscriminatorStack = [\n",
        "            UnconditionalDiscriminator(4, (5, 3)),\n",
        "            UnconditionalDiscriminator(8, (5, 3)),\n",
        "            UnconditionalDiscriminator(15, (2, 2))]\n",
        "        self.cDiscriminatorStack = [\n",
        "            ConditionalDiscriminator(4, (1, 5, 3, 2, 2)),\n",
        "            ConditionalDiscriminator(8, (1, 5, 3)),\n",
        "            ConditionalDiscriminator(15, (1, 2, 2, 2))  \n",
        "        ]\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.denseStack = ([tf.keras.layers.Dense(1) for i in range(3)])\n",
        "    \n",
        "    def call(self, w1Inputs, w2Inputs, w3Inputs, condition):\n",
        "        outputs = 0\n",
        "        windows = [w1Inputs, w2Inputs, w3Inputs]\n",
        "        for uDisc, cDisc, window, dense in zip(self.uDscriminatorStack, self.cDiscriminatorStack, windows, self.denseStack):\n",
        "            outputs += dense(self.flatten(uDisc(window)) + self.flatten(cDisc(window, condition)))\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PsaKju9TaTi"
      },
      "source": [
        "# GAN test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ItCTvRiq9xW"
      },
      "source": [
        "DISC_LEARNING_RATE = 1e-4\n",
        "GEN_LEARNING_RATE = 5e-5\n",
        "BETA_1 = 0\n",
        "BETA_2 = 0.999\n",
        "DECAY_RATE = 0.9999\n",
        "PREPROCESSOR = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
        "ENCODER = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1\"\n",
        "BERT_MODEL = BERT(PREPROCESSOR, ENCODER)\n",
        "WINDOWS = [240, 480, 960, 1920, 3600]\n",
        "WINDOWS_TEST = [960, 1920, 3600]\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLdHIBYyUqWl"
      },
      "source": [
        "def getSamples(audioArray, windows):\r\n",
        "    totalSamples = len(audioArray[0])\r\n",
        "    subSamples = []\r\n",
        "    for window in windows:\r\n",
        "        idx = np.random.randint(0, totalSamples - window)\r\n",
        "        subSamples.append(audioArray[:, idx:idx+window, :])\r\n",
        "    return subSamples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz3VtAbSrI6l"
      },
      "source": [
        "def initializeModels():\n",
        "    featureNet = CBHG(BATCH_SIZE, 16, True)\n",
        "    generator = Generator(BATCH_SIZE, True)\n",
        "    discriminatorTest = DiscriminatorTest()\n",
        "    #discriminator = Discriminator()\n",
        "    genOptimizer = tfa.optimizers.MovingAverage(decay=DECAY_RATE, optimizer=tf.keras.optimizers.Adam(lr=GEN_LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2))\n",
        "    discOptimizer = tf.keras.optimizers.Adam(lr=DISC_LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2)\n",
        "    featureOptimizer = tf.keras.optimizers.Adam(lr=DISC_LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2)\n",
        "    return featureNet, generator, discriminatorTest, genOptimizer, discOptimizer, featureOptimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3MJRHDRpMxn"
      },
      "source": [
        "def trainStep(audioBatch, textBatch, featureNet, generator, discriminator, genOptimizer, discOptimizer, featureOptimizer):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        noise = tf.random.normal((BATCH_SIZE, 128, 1))\n",
        "        with tf.GradientTape() as genTape, tf.GradientTape() as discTape, tf.GradientTape() as featureTape:\n",
        "            genFeatures, discFeatures = featureNet(textBatch)\n",
        "            generatedAudio = generator(genFeatures, noise)\n",
        "            # w1, w2, w3, w4, w5 = getSamples(generatedAudio, WINDOWS)\n",
        "            # fakeAudio = discriminator(w1, w2, w3, w4, w5, discFeatures)\n",
        "            w3, w4, w5 = getSamples(generatedAudio, WINDOWS_TEST)\n",
        "            fakeAudio = discriminator(w3, w4, w5, discFeatures)\n",
        "            # w1, w2, w3, w4, w5 = getSamples(audioBatch, WINDOWS)\n",
        "            # realAudio = discriminator(w1, w2, w3, w4, w5, discFeatures)\n",
        "            w3, w4, w5 = getSamples(audioBatch, WINDOWS_TEST)\n",
        "            realAudio = discriminator(w3, w4, w5, discFeatures)\n",
        "            discFakeLoss = tf.losses.hinge(tf.zeros_like(fakeAudio), fakeAudio)\n",
        "            discRealLoss = tf.losses.hinge(tf.ones_like(realAudio), realAudio)\n",
        "            discLoss = discFakeLoss + discRealLoss\n",
        "            genLoss = tf.losses.hinge(tf.ones_like(fakeAudio), fakeAudio)\n",
        "        discGradients = discTape.gradient(discLoss, discriminator.trainable_variables)\n",
        "        discOptimizer.apply_gradients(zip(discGradients, discriminator.trainable_variables))\n",
        "        genGradients = genTape.gradient(genLoss, generator.trainable_variables)\n",
        "        genOptimizer.apply_gradients(zip(genGradients, generator.trainable_variables))\n",
        "        featureGradients = featureTape.gradient(discLoss, featureNet.trainable_variables)\n",
        "        featureOptimizer.apply_gradients(zip(featureGradients, featureNet.trainable_variables))\n",
        "        print(\"Generator loss:\", genLoss.numpy()[0],\"| Discriminator loss:\", discLoss.numpy()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c131H79qCMf"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "    featureNet, generator, discriminator, genOptimizer, discOptimizer, featureOptimizer = initializeModels()\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch\", epoch+1)\n",
        "        for batch in dataset:\n",
        "            trainStep(batch[0], batch[1], featureNet, generator, discriminator, genOptimizer, discOptimizer, featureOptimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjK5077DTc5F"
      },
      "source": [
        "audio = tf.random.normal((2, 48000, 1))\r\n",
        "text_input = ['This is such an amazing movie!', 'English is a West Germanic language first spoken in early medieval England']\r\n",
        "text = BERT_MODEL(text_input)\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((audio, text)).batch(2)\r\n",
        "train(dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}